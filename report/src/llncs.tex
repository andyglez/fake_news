\documentclass{llncs}

%
\usepackage{geometry} % to change the page dimensions
\usepackage{natbib}


\geometry{a4paper}

\begin{document}


\title{Detecci\'on autom\'atica de noticias falsas}

\author{Andy Gonz\'alez Pe\~na \\ a.gonzalez@estudiantes.matcom.uh.cu}
\institute{Matem\'atica y Computaci\'on, Universidad de la Habana, 2018}

\maketitle


\begin{multicols}{2}[]

\begin{abstract}

Con el auge de las redes sociales el consumo de informaci\'on negativa ha aumentado exponencialmente al punto que se requieren de nuevos mecanismos
que, al menos, regulen su circulaci\'on. Encabezando la lista se encuentran las noticias falsas que, a pesar de siempre haber existido ya sea por fines pol\'iticos
u otros, toman nueva relevancia al aparentar pertenecer a canales oficiales o aparecer en la red de confianza de un usuario de una determinada red social.
En este trabajo se presentan las caracter\'isticas principales del problema antes de la aplicaci\'on en la web que se conoce en la actualidad, es decir, se propone
como reconocer patrones de una noticia desde el punto de vista est\'atico en el que no se puede hacer nada por no recibir tal noticia, todo lo contrario, se recibe
y se decidir\'a la validez a partir de un an\'alisis utilizando t\'ecnicas de \textit{Machine Learning} (ML).

\end{abstract}


\section{Introducci\'on}\label{sec:Introduction}

Muchos esfuerzos son dedicados en la actualidad para enfrentar la sombra que proyectan las divulgaciones falsas y negativas sobre el sano uso de las redes sociales. 
En este trabajo se resumen las principales t\'ecnicas existentes para resolver esta problem\'atica as\'i como las diferencias entre los resultados aplicados a modelos que 
solamente dependen de la noticia en cuesti\'on como las propuestas enfocadas a las redes sociales, esto se puede encontrar en la secci\'on de \textbf{Estado del Arte}.

Tomando en cuenta el an\'alisis arriba mencionado se proceder\'a a proponer una soluci\'on, en la secci\'on de igual nombre, basada en t\'ecnicas de ML con datos como
titular y texto exclusivamente y utilizando un \textit{corpus} de clasificaci\'on en solamente dos categor\'ias, reales o falsas, utilizando las principales caracter\'isticas del
procesamiento de lenguaje natural (NLP) que no depender\'a de entidades externas como en una plataforma social, es decir, autores o referencias, sino utilizando \'unicamente 
los patrones m\'as abundantes dentro del texto. En consecuencia, se ofrecer\'an los resultados alcanzados y la relevancia de los mismos en comparaci\'on con los sistemas 
ya existentes en explotaci\'on.

Dentro de la rama de miner\'ia de datos, la detecci\'on autom\'atica de noticias falsas se encuentra a\'un en sus inicios y existen muchos trabajos y discusiones abiertas,
por tanto, la propuesta de soluci\'on no deber\'a ser tomada a la ligera ni con el af\'an de implementaci\'on \'optima. De manera concluyente se presentan aspectos te\'oricos
que podr\'ian brindar mejores resultados as\'i como futuras incorporaciones que deber\'an ser aplicadas con el objetivo de proponer un sistema altamente certero.

Con este trabajo se persigue la consolidaci\'on de los conocimientos b\'asicos de miner\'ia de datos, inteligencia artificial y sistemas de informaci\'on en cuesti\'on, la aplicaci\'on 
de esquemas de procesamiento de lenguaje natural (NLP) y abrir nuevos debates constructivos que impacten en un grado positivo sistemas de estas caracter\'isticas para, 
con ello, poder disfrutar en mayor medida de la parte sana de la actual difusi\'on de la informarci\'on.

Sin pretensiones de alta profundizaci\'on en diversas tem\'aticas especialmente aplicadas a las redes sociales, o la propia definici\'on de noticias falsas, como pueden ser factores
 psicol\'ogicos, pol\'iticos, sociales u otros, se procede a continuar con la siguiente secci\'on, ce\~nida a elementos puramente computacionales.

\section{Estado del Arte}

A pesar de ser una tem\'atica relativamente nueva, en consecuencia de relevancia y de lo que apremia como problema social, existen numerosos trabajos que investigan e incluso
ya explotan los bases del conocimiento adquirido. Sitios reportan hasta 90 \% de efectividad para clasificar una noticia en verdadera o falsa y para ello se bastan de las t\'ecnicas
de NLP y ML, sin embargo utilizan otros datos relevantes a su entorno, datos sociales como pueden ser fuentes y referencias, o grado de pertenencia a grupos de confiabilidad en
cuanto informaci\'on, entre otras.

Los sistemas que indican un menor porcentage de efectividad analizan puramente texto, y hasta el momento la cifra m\'as elevada se encuentra en 76 \%. Parecer\'ia una cifra
no tan alta, sin embargo, estudios indican que los humanos se equivocan el 70 \% de los casos en definir la veracidad de una noticia. Esto significa que para ser un sistema que
automatiza el trabajo exhaustivo de analistas y que propone mejores resultados que ellos no est\'a nada mal.

\subsection{Fundamentos NLP}

Se define la problem\'atica como una situaci\'on espec\'ifica de ML, se tiene una base de conocimientos dados por noticias y sus clasificaciones y se pretende dise\'nar un modelo
que aprenda de ello y sea capaz, entonces, de predecir nuevos resultados a partir de lo que ya sabe. La ciencia de los datos se divide en dos partes cuando acontece un problema
de este tipo: primeramente se conocen los modelos existentes de ML para afrontarlo, pero sus entradas son vectores de datos y la base de conocimientos solamente posee texto, 
es precisamente esa la segunda cuesti\'on y quiz\'as la m\'as importante del problema, la extracci\'on de datos relevantes para que el modelo propuesto clasifique correctamente.

Entonces, descartando los datos que podr\'ian ser relevantes para los sistemas de manera general, quedamos solamente con los que podr\'ia ofrecer un texto dentro de la base del
conocimiento, as\'i se definen los patrones linguisticos del lenguaje natural, los cuales se dividen en tres grupos fundamentales: lexicogr\'aficos, sint\'acticos y sem\'anticos. Los 
elementos lexicogr\'aficos pueden ser, por ejemplo, una tabla de frecuencias de palabras con lo que se pueden denotar estilos en los usos de palabras dentro del texto. Los elementos
sint\'acticos podr\'ian ser una tabla de frecuencias de frases u oraciones gramaticales con lo que se pueden denotar secciones relevantes a la noticia en su completitud. Por \'ultimo,
tenemos los elementos sem\'anticos del texto, estos podr\'ian tomar la forma de similaridad espacio-vectorial entre oraciones.

Los problemas que NLP contiene son mucho m\'as que lo que aborda este trabajo, sin embargo en cuanto a la extracci\'on de los elementos antes mencionados existen ya muchas
t\'ecnicas.
La tokenizaci\'on por oraciones y por palabras son soluciones que hoy podemos llamar \textit{straight-forward} y son claves para el resto del an\'alisis. Para la parte sint\'actica, la
herramienta m\'as com\'un es la utilizaci\'on de \textit{POS-tagging}, basado en la implementaci\'on de un perceptr\'on pre-entrenado de la librer\'ia de su pertenencia para reconocer
\textit{Parts-of-Speech} o partes del texto apoy\'andose en una gram\'atica para extraer las que se consideran relevantes al problema. En este tema existen grandes debates acerca
de que parte del texto representa en mayor grado su oraci\'on o su texto propiamente. Para la extracci\'on de elementos sem\'anticos la pr\'actica m\'as com\'un est\'a basada en la 
similaridad espacio-vectorial la cual, a su vez, se apoya en la t\'ecnicas como esquema de N-gramas o \textit{Bag-of-Words}. Estas t\'ecnicas vectorizan y cuantifican la relaci\'on de una 
frase de tama\~no n, o un conjunto de palabras, en su \'ambito oracional.

\subsection{Fundamentos Aplicados ML}

El modelo de ML a utilizar es la otra secci\'on del dise\~no, sin embargo, cual sea el escogido, todos los vectores y matrices recogidos utilizando las t\'ecnicas vistas anteriormente deben
ser normalizados y recontorneados para acomodarse al modelo; por ejemplo, si se ten\'ia como resultado una matriz y se requiere de un conjunto de datos plano, entonces la primera ser\'a
aplanada, es decir, convertida a un vector.

La pr\'actica com\'un para la detecci\'on de noticias falsas utiliza los algoritmos de redes neuronales para el entrenamiento o aprendizaje del modelo de clasificaci\'on en falsas o verdaderas.
Sin embargo, las categor\'ias oficiales de estos sistemas son: basados en conocimientos y basados en estilos. Los primeros se acercan mucho a la problem\'atica en la web y como enfocarlo
desde tal punto de vista, mientras que la detecci\'on de estilos propone un profundo an\'alisis textual para definir la veracidad de la noticia. Para la detecci\'on de estilos se han utilizado,
incluso, redes convolucionales dependiendo de la magnitud de datos a procesar.

\section{Propuesta de Soluci\'on}

Dadas las caracter\'isticas del \textit{corpus} a utilizar, el cual contiene t\'itulo, cuerpo y clasificaci\'on, se propone la implementaci\'on de un modelo basado en estilos para insertar
profundidad en el an\'alisis textual de la base de conocimientos. En consecuencia, el procedimiento de ML se llevar\'a aplicando las secuencias de entrenamiento, validaci\'on y predicci\'on
que conlleva el uso de los algoritmos de redes neuronales, sin llegar al despliegue total de las redes convolucionales. Junto a ello, para la extracci\'on de \textit{features} se procede
a la implementaci\'on de los aspectos antes mencionados de NLP, es decir, el an\'alisis lexicogr\'afico, sint\'actico y sem\'antico.

\subsection{Especificidades y Dependencias}

La soluci\'on se ha escrito en Python (v. 3.6.1) de la distribuci\'on de Anaconda y se utilizan diferentes librer\'ias para cumplir los objetivos planteados. Para la implementaci\'on de redes
neuronales, se utiliza la librer\'ia de \textit{Tensorflow} y dentro, espec\'ificamente, el \textit{framework} de \textit{Keras} el cual provee un mayor nivel de abstracci\'on en la resoluci\'on
modular de los problemas de ML. Para los an\'alisis de patrones lingu\'isticos se utiliza NLTK, herramienta poderosa que contiene muchas sub-librer\'ias de alto valor funcional,
espec\'ificamente y aplicado a este trabajo, se denotan las secciones de tokenizaci\'on, de POS-tagging y extracci\'on gramatical. Mientras, para la vectorizaci\'on en el c\'alculo de
similaridad, se utiliza la librer\'ia de \textit{sklearn}, en las secci\'on de extracci\'on de \textit{features}.

\subsection{Estructura y Funcionalidades}

Se comienza con la carga de la base de conocimientos y se procede a el an\'alisis de patrones existentes en la misma. Dado que el proyecto va enfocado al procesamiento de textos,
contin\'ua a la extracci\'on de caracter\'isticas puntuales o de estilos dentro de los textos de noticias, para este caso se denominan los \textit{features} lingu\'isticos.

Dentro de las caracter\'isticas lingu\'isticas, el an\'alisis se divide en tres: lexicogr\'afico, sint\'actico y sem\'antico. En el primero se pretende encontrar el uso o frecuencias de palabras
dentro de el texto de la noticia, y con ello proponer la primera caracter\'istica de estilos: probabilidad de que una noticia sea falsa si el estilo de repetici\'on de palabras contiene unas
frecuencias de $x_1$, $x_2$, ..., $x_n$. De la misma manera, el an\'alisis sint\'actico pretende encontrar las repeticiones de frases que se asumen que tienen peso dentro de cada
oraci\'on del texto como pueden ser las nominaciones (sustantivos) con sus calificaciones (adjetivos) junto al posible uso de art\'iculos, entre otros; as\'i se construye otra tabla de
frecuencias y se propone otra caracter\'istica de estilos: probabilidad de que una noticia sea falsa si el estilo de repetici\'on de frases contiene unas frecuencias de $y_1$, $y_2$, ...,
$y_n$. Por \'ultimo, el an\'alisis sem\'antico procede al c\'alculo de la similaridad entre oraciones, es decir, se analiza las relaciones de concordancia en el texto, entonces se obtiene
la probabilidad de que una noticia sea falsa dado que su estilo contiene tales grados de concordancia a la idea general que indica, esto se calcula mediante combinaciones de frases
dentro de las mismas oraciones seg\'un el modelo de N-gramas, frecuencias y sus inversas (tf*idf) para denotar su peso en la idea y la normalizaci\'on matricial para computar el grado 
de similaridad.

Una vez extra\'idos los principales patrones se obtienen de ellos matrices de datos relevantes para la clasificaci\'on, sin embargo, estas pueden ser de tama\~nos variables y, por ende,
es necesario llevarlas todas a la misma medida de tal forma que se conforme la descripci\'on vectorial de una noticia. Una vez computado el vector de cada patr\'on se contin\'ua a la 
combinaci\'on de ellos mediante la concatenaci\'on de listas.

Estos vectores conformar\'an los verdaderos conjuntos de entrenamiento y validaci\'on para las redes neuronales. Con el objetivo de lograr esto, se utiliza el m\'etodo de \textit{k-fold},
espec\'ificamente \textit{2-fold} en el que se divide el \textit{corpus} en dos conjuntos: los llamados entrenamiento y validaci\'on. En el primero se ajusta el modelo a minimizar la funci\'on
objetivo que describe el conjunto de datos y la segunda se dedica a evaluar el \'indice de correctitud de las predicciones del sistema.

\section{Resultados}


\end{multicols}

\bibliographystyle{apa}
\bibliography{ref}


\end{document}